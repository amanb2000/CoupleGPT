"""Business logic for the inference server."""

from typing import Dict, Any, List
import requests
import threading
import queue

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import public_ip as ip

from languagegame.models import GenRequest, LossRequest, InferenceServerModel, GenResponse
from languagegame import compute_score



# Helper class -- used only by the inference server to keep track of requests. 
# This extends the GenRequest class with a threading.Event() and a result field. 
# The threading.Event() is used to signal when the request has been processed.
class RequestItem:
   def __init__(self, data):
       """ `data` is either of type GenRequest or LossRequest
       """
       self.gen_request = data # the original `GenRequest` object
       self.event = threading.Event()
       self.result = None

class InferenceServer:
    """
    Business logic for the inference server.
    
    Provides methods for text generation and computing cross-entropy loss.
    """

    def __init__(self, port:int, 
                 model_name: str = "tiiuae/falcon-7b", 
                 coordinator_url: str = "http://lancelot.languagegame.io:8000", 
                 host: str = "0.0.0.0", 
                 max_seq_len:int=-1, 
                 batch_size:int = 1):
        # Loading the model and tokenizer
        if model_name is None or model_name == "None": 
            self.model_name = "None"
            print("WARNING: NO MODEL NAME PROVIDED. USE THIS FEATURE ONLY FOR TESTING.")
        else: 
            print(f"Loading model and tokenizer `{model_name}` for the InferenceServer class...")
            self.model_name = model_name
            self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, 
                                                        add_bos_token=False, 
                                                        add_eos_token=False)
            self.tokenizer.pad_token = self.tokenizer.eos_token


            # Moving model to appropriate device
            self.model.eval()

            # run in float16
            # self.model = self.model.to(torch.bfloat16)
            print("Done loading model and tokenizer.\n\n")
        
        self.port = port
        self.coordinator_url = coordinator_url
        self.host = host
        self.max_seq_len = max_seq_len
        self.uid = "None" # assigned during registration with the coordinator
        self.ip_addr = str(ip.get()) 
        self.cond_var = threading.Condition()
        self.model_cond_var = threading.Condition()
        self.device = 'cuda'


        if model_name != "None": 
            # Convert the blacklisted phrases to their respective token IDs
            self.init_blacklist()


        self.generate_queue = queue.Queue()
        self.loss_queue = queue.Queue()
        self.last_action = "None" # None, Generate, Loss


        self.batch_size = batch_size

        # Registering with the coordinator
        self.register_with_coordinator()

    def init_blacklist(self): 
        """ Populates the `self.bad_word_ids` with a list of tokens that should
        not be generated by the model. 
        """
        self.blacklisted_phrases = [
            "[agent", 
            "[agent]",
            "[ agent ]", 
            "[Agent]", 
            "[AGENT]",
            "[AGENT]:",
            "[Agent]: ",
            "[RECENT CONTEXT]", 
            "[RECENT CORPUS CHUNK]", 
            "[CURRENT DISCUSSION WITH", 
            "(me)", 
            "( me )", 
            "(me):",
            "[CURRENT DISCUSSION",
            "agent"
            "RECENT DISCUSSION"
        ]

        # add all versions of blacklisted phrases with a leading space
        self.blacklisted_phrases += [" " + phrase for phrase in self.blacklisted_phrases]
        # add all versions of blacklisted phrases with tailing space
        self.blacklisted_phrases += [phrase + " " for phrase in self.blacklisted_phrases]


        self.bad_word_ids = [self.tokenizer.encode(phrase) for phrase in self.blacklisted_phrases]


        # Now let's add any token with '[' or ']' in it
        illegal_chars = ['[', ']', '(', ')', 'agent', 'agents', 'Agent', 'AGENT', 'FAILURE', 'failure', ':', '\\n', 'concourse', '/r/', '<', '>', '"', "'"]
        for i in range(self.tokenizer.vocab_size): 
            i_str = self.tokenizer.decode([i])
            for char in illegal_chars:
                if char in i_str: 
                    self.bad_word_ids.append([i])
                    break

        for bad_ids in self.bad_word_ids: 
            print("Bad word ids: ", bad_ids)
            print("Decoded bad word ids: ", self.tokenizer.decode(bad_ids))


    def get_self_model(self):
        """ Returns an InferenceServerModel object representing this InferenceServer.
        """
        return InferenceServerModel(
            uid=str(self.uid), 
            ip_address=str(self.ip_addr),
            port=self.port,
            llm_name=str(self.model_name),
            max_seq_len=self.max_seq_len, 
            batch_size=self.batch_size
        )

    def register_with_coordinator(self):
        """ Register with the coordinator. We expect to receive an 
        InferenceServerModel object in response with our new UID (we should
        set this)
        """
        print("Registering with the coordinator at ", self.coordinator_url)
        response = requests.post(self.coordinator_url + "/register_inference_server", json=self.get_self_model().model_dump())
        if response.status_code == 200:
            print("Successfully registered with the coordinator.")
            self.uid = response.json()["uid"]
            # assert that the rest of the fields match 
            assert self.ip_addr == response.json()["ip_address"]
            assert self.port == response.json()["port"]
            assert self.model_name == response.json()["llm_name"]
            assert self.max_seq_len == response.json()["max_seq_len"]
        else:
            raise Exception("Failed to register with the coordinator. Response = {}".format(response.json()))

    def unregister_from_coordinator(self): 
        """ Unregister from the coordinator. 
        """
        print("Unregistering from the coordinator at ", self.coordinator_url)
        response = requests.post(self.coordinator_url + "/unregister_inference_server", json=self.get_self_model().model_dump())
        if response.status_code == 200:
            print("Successfully unregistered from the coordinator.")
        else:
            raise Exception("Failed to unregister from the coordinator. Response = {}".format(response.json()))

        
    def worker_daemon(self): 
        """ The worker daemon runs in a separate thread and processes requests 
        from the generate_queue. This allows the inference server to process 
        multiple requests simultaneously and leverage the batch dimension of 
        the LLM to perform inference more efficiently.
        """
        while True: # main loop 
            print("I AM THE WORKER THREAD. I AM AWAKE.")
            with self.cond_var: 
                while self.generate_queue.qsize() == 0 and self.loss_queue.qsize() == 0: 
                    self.cond_var.wait()
                # Retrieve the next request(s) from the queue 

                if self.loss_queue.qsize() == 0 or ( (self.generate_queue.qsize() > 0 and self.loss_queue.qsize() > 0) and (self.last_action == "None" or self.last_action == "Loss") ): 
                    assert self.generate_queue.qsize() > 0 
                    # We must deal with the generate queue first. 
                    print("Worker thread activated -- processing the GENERATE QUEUE.")
                    batch = [] 
                    while not self.generate_queue.empty() and len(batch) < self.batch_size:
                        # TODO: add logic for discriminating between CE loss and 
                        # generate requests
                        batch.append(self.generate_queue.get_nowait())
                    self.process_generation_batch(batch)
                    self.last_action = "Generate"
                elif self.loss_queue.qsize() > 0: 
                    # We must deal with the loss queue first. 
                    print("Worker thread activated -- processing the LOSS QUEUE.")
                    # for now we will just get the next element in the loss queue 
                    # and process it with the CE loss endpoint
                    request = self.loss_queue.get_nowait()
                    response = self.compute_loss(request.gen_request) # Perform computation
                    request.result = response # Store the result
                    request.event.set() # notify the waiting endpoint thread
                    self.last_action = "Loss"
                else: 
                    raise Exception("Invalid last action: {}".format(self.last_action))
            # process the batch of requests

    def _encode_generation_batch(self, batch: List[RequestItem]): 
        """ Encodes a batch of generation requests with our special truncation 
        strategy for the system_prompt and the input_ids. Pads all sequences
        to the same length. 
        """
        max_length = -1
        input_ids_list = []
        num_truncated_list = []
        num_pads_list = []
        for request_item in batch:
            request = request_item.gen_request
            system_prompt_ids = self.tokenizer.encode(request.system_prompt, return_tensors="pt").to(self.device)
            input_ids = self.tokenizer.encode(request.input_string, return_tensors="pt").to(self.device)
            num_truncated = 0
            if self.max_seq_len != -1 and system_prompt_ids.shape[1] + input_ids.shape[1] + request.num_tokens > self.max_seq_len:
                # warning: truncating the input string to fit context+input+generated into max_seq_len 
                print("WARNING: truncating the input string to fit context+input+generated into max_seq_len")
                num_input_ids = self.max_seq_len - system_prompt_ids.shape[1] - request.num_tokens
                num_truncated = input_ids.shape[1] - num_input_ids
                input_ids = input_ids[:, -num_input_ids:]
            
            # Concatenate the system prompt and input string
            input_ids = torch.cat([system_prompt_ids, input_ids], dim=1)
            # convert to longtensor
            input_ids = input_ids.long()
            length = input_ids.shape[1]
            if length > max_length:
                max_length = length
            input_ids_list.append(input_ids)

            num_truncated_list.append(num_truncated)
        
        # Pad all sequences to the same length
        print("Pad token: ", self.tokenizer.pad_token_id)
        for i in range(len(input_ids_list)):
            input_ids = input_ids_list[i]
            pad_length = max_length - input_ids.shape[1]
            num_pads_list.append(pad_length)
            input_ids_list[i] = torch.cat([torch.ones([1, pad_length]).long().to(self.device) * self.tokenizer.pad_token_id, input_ids.to(self.device)], dim=1).to(self.device)
            # type check -- should be longtensor
            assert input_ids_list[i].dtype == torch.long
            assert input_ids_list[i].shape[1] == max_length
        
        # Concatenate all the input_ids into a single tensor
        input_ids = torch.cat(input_ids_list, dim=0)
        return input_ids, num_truncated_list, num_pads_list

    def process_generation_batch(self, batch: List[RequestItem]): 
        """ Process a batch of generate requests. For now, we will just 
        transplant the logic from `generate()` and make sure the batch size 
        is 1. 
        """
        assert len(batch) <= self.batch_size and len(batch) >= 1
        print("BATCH SIZE: ", len(batch))
        batch_ids, num_truncated_list, num_pads_list = self._encode_generation_batch(batch)
        attention_mask = torch.ones(batch_ids.shape).to(self.device)
        for num_pads in num_pads_list:
            attention_mask[:, :num_pads] = 0

        print("batch_ids.shape = ", batch_ids.shape)

        # We can only generate one length for all of them -- they had better 
        # match if they're all playing the same game!
        for req in batch: 
            assert req.gen_request.num_tokens == batch[0].gen_request.num_tokens

        responses = self.generate(batch_ids, batch[0].gen_request.num_tokens, attention_mask) # response: GenResponse

        # Returning the results to the clients.
        assert len(responses) == len(batch)
        for i in range(len(responses)): 
            responses[i].num_truncated = num_truncated_list[i]
            batch[i].result = responses[i]
            batch[i].event.set()
        return 
                    
    def generate_endpoint(self, request: GenRequest): 
        """ Endpoint for the generate API. Adds the 
        request to the queue and signals the condition variable. 
        """
        print("Storing incoming GENERATE request in the request queue...")
        request_item = RequestItem(request)
        with self.cond_var: 
            self.generate_queue.put(request_item)
            self.cond_var.notify()
        print("Done storing request in the request queue. Now waiting for the request to be processed by the worker thread...")
        
        # Wait for the request to be processed
        processed = request_item.event.wait(30)

        if not processed: 
            raise Exception(status_code=408, detail="Request timed out")

        print("Request processed by the worker thread! Returning results to client.")
        return request_item.result

    def ce_loss_endpoint(self, request: LossRequest): 
        """ Endpoint for the CE loss API. Adds the 
        request to the queue and signals the condition variable. 
        """
        print("Storing incoming CE LOSS request in the request queue...")
        request_item = RequestItem(request)
        with self.cond_var: 
            self.loss_queue.put(request_item)
            self.cond_var.notify()
        print("Done storing request in the request queue. Now waiting for the request to be processed by the worker thread...")
        
        # Wait for the request to be processed
        processed = request_item.event.wait(30)

        if not processed: 
            raise Exception(status_code=408, detail="Request timed out")

        print("Loss processed by the worker thread! Returning results to client.")
        return request_item.result

    def generate(self, batch_ids: torch.Tensor, num_tokens: int, attention_mask: torch.Tensor):
        """
        Generate text based on the provided batch of input strings. 

        Args:
         - batch_ids: the tokenized input strings we are generated off of. 
            Shape: [batch_size, < max_seq_len]. Must be padded/truncated 
            already.
         - num_tokens: the number of tokens to generate for each input string.
        Returns:
         - responses: a list of GenResponse objects.
        """
        if self.model_name == "None": 
            raise Exception("No model loaded. Use this feature only for testing.")

        try:
            # Generate text using the model
            with self.model_cond_var: 
                with torch.no_grad():
                    generated_ids = self.model.generate(batch_ids, 
                                                        max_length=batch_ids.shape[1] + num_tokens, 
                                                        # CONTRASTIVE DECODING -- Didn't work that well for GPT-2 #
                                                        # penalty_alpha=0.6, top_k=4, # contrastive decoding
                                                        no_repeat_ngram_size=2,  # To ensure more diversity in the generated text

                                                        do_sample=True,
                                                        # top_k=10,
                                                        temperature=0.8,

                                                        attention_mask=attention_mask,
                                                        bad_words_ids=self.bad_word_ids  # Blacklist phrases
                                                        ).to(self.device)

            # select only the text after the initial input string
            generated_ids = generated_ids[:, batch_ids.shape[-1]:]
            
            # Decode the generated ids to string
            responses = []
            for i in range(generated_ids.shape[0]):
                generated_text = self.tokenizer.decode(generated_ids[0, :], skip_special_tokens=True)
                input_string = self.tokenizer.decode(batch_ids[i, :], skip_special_tokens=True)

                generated_response = GenResponse(
                    input_string = input_string,
                    generated = generated_text, 
                    num_truncated = -1
                )

                responses.append(generated_response)

            return responses 
        except Exception as e:
            print("Exception in inference_server.py@381: ", e)
            raise e  # Re-raising the exception to be caught and handled by FastAPI.


    def compute_loss(self, request: LossRequest) -> Dict[str, Any]:
        """
        Compute the cross-entropy loss for a given context and corpus string.

        Args:
        - request (LossRequest): Contains context and corpus strings.

        Returns:
        - Dict with the original request and computed loss.
        """
        if self.model_name == "None": 
            raise Exception("No model loaded. Use this feature only for testing.")

        try:
            with self.model_cond_var: 
                context_ids = self.tokenizer.encode(request.context_string, return_tensors="pt").to(self.device)
                corpus_ids = self.tokenizer.encode(request.corpus_string, return_tensors="pt").to(self.device)

                input_ids = torch.cat([context_ids, corpus_ids], dim=1)

                loss_mask = torch.ones([1, context_ids.shape[1] + corpus_ids.shape[1]]).to(self.device)
                loss_mask[:, :context_ids.shape[1]] = 0

                assert loss_mask.shape == input_ids.shape, "Loss mask and input_ids must have the same shape."
                loss = compute_score(input_ids, loss_mask, self.model, self.tokenizer)
                
                return {
                    "initial_request": {
                        "context_string": request.context_string,
                        "corpus_string": request.corpus_string
                    },
                    "loss": loss.item()
                }
        except Exception as e:
            raise e  # Re-raising the exception to be caught and handled by FastAPI.
